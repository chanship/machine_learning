{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours\n",
    "\n",
    "In this task, I will use four distance functions: (we removed the vector symbol for simplicity)\n",
    "\n",
    "- Euclidean distance:  $$d(x, y) = \\sqrt{\\langle x - y, x - y \\rangle}$$\n",
    "- Inner product distance: $$d(x, y ) = \\langle x, y \\rangle$$\n",
    "- Gaussian kernel distance: \n",
    "    $$d(x, y ) = - \\exp({−\\frac 12 \\langle x - y, x - y \\rangle}) $$\n",
    "- Cosine Distance: $$d(x, y) =\\cos(\\theta )={\\mathbf {x} \\cdot \\mathbf {y}  \\over \\|\\mathbf {x} \\|\\|\\mathbf {y} \\|}$$\n",
    "\n",
    "F1-score is a important metric for binary classification, as sometimes the accuracy metric has the false positive (a good example is in MLAPP book 2.2.3.1 “Example: medical diagnosis”, Page 29).\n",
    "We have provided a basic definition. For more you can read 5.7.2.3 from MLAPP book.\n",
    "\n",
    "<img src=\"F1Score.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1.1 Distance Functions\n",
    "\n",
    "Implement the class in file hw1_knn.py\n",
    "    the functions in utils.py    \n",
    "    - f1_score\n",
    "    - euclidean_distance\n",
    "    - inner_product_distance\n",
    "    - gaussian_kernel_distance\n",
    "    - cosine distance\n",
    "\n",
    "Simply follow the formula above and finish all these function. You are not allowed to call any package that we did not import for you.\n",
    "    \n",
    "### Part 1.1.2 KNN Class\n",
    "\n",
    "There are following functions you need to implement in KNN class:\n",
    "\n",
    "1.def train(self, features: List[List[float]], labels: List[int])\n",
    "     \n",
    "In this function, features are simply all training data which is a 2D list with float. For example, if the data looks like the following: Student 1 with features age 25, grade 3.8 and labeled as 0, Student 2 with features age 22, grade 3.0 and labeled as 1, then the feature data should be [ [25.0, 3.8], [22.0,3.0] ], thus the coresponding label is [0,1]\n",
    "    \n",
    "For KNN, the training process is just loading all the training data. Thus, all you need to do in this function is create some local variable in KNN class to store these data so you can use the data in later process.\n",
    "    \n",
    "2.def get_k_neighbors(self, point: List[float]) -> List[int]:\n",
    "\n",
    "This function takes one single data point and ask you to find the nearest k neighbours in the training set. You already have your k value, distance function and you just stored all training data in KNN class with the train function. \n",
    "\n",
    "This function need to return a list of labels of all k neighours.\n",
    "\n",
    "3.def predict(self, features: List[List[float]]) -> List[int]\n",
    "\n",
    "The predict function take another 2D list which is all testing data point, Similar to those from train function. In this function, you need process every testing data point, reuse the get_k_neighbours function to find the nearest k neighbours for each testing data point, find the majority of labels for these neighbours as the predict label for that testing data point. Thus, you will get N predicted label for N testing data point.\n",
    "\n",
    "This function need to return a list of predicted labels for all testing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hw1_knn import KNN\n",
    "from utils import euclidean_distance, gaussian_kernel_distance, inner_product_distance, cosine_sim_distance\n",
    "from utils import f1_score, model_selection_without_normalization, model_selection_with_transformation\n",
    "distance_funcs = {\n",
    "    'euclidean': euclidean_distance,\n",
    "    'gaussian': gaussian_kernel_distance,\n",
    "    'inner_prod': inner_product_distance,\n",
    "    'cosine_dist': cosine_sim_distance,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data_processing\n",
    "Xtrain, ytrain, Xval, yval, Xtest, ytest = data_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1.3 Model selection \n",
    "\n",
    "In this section, you need to implement the following function:\n",
    "\n",
    "1.def model_selection_without_normalization(distance_funcs, Xtrain, ytrain, Xval, yval)\n",
    "\n",
    "In this part, you should try different distance function you implemented in part 1.1, and find the best k. Use k range from 1 to 30 and increment by 2. We will use f1-score to compare different models. \n",
    "\n",
    "THis function take the following parameter:\n",
    "\n",
    "distance_funcs: dictionary of distance funtion you will use to calculate the distance. Make sure you loop over all distance function for each data point and each k value.\n",
    "\n",
    "Xtrain: List[List[int]] training data set to train your KNN model\n",
    "\n",
    "ytrain: List[int] train labels to train your KNN model\n",
    "\n",
    "Xval: List[List[int]] validation data set you will use on your KNN predict function to produce predicted labels and tune k and distance function.\n",
    "\n",
    "yval: List[int] validation labels\n",
    "\n",
    "This function need to return the following:\n",
    "\n",
    "best_model: an instance of KNN\n",
    "\n",
    "best_k: best k choosed for best_model\n",
    "\n",
    "best_func: name of best function choosed for best_model\n",
    "\n",
    "Thus, the function only return one set of  model, k and function.\n",
    "\n",
    "chose model based on the following priorities:\n",
    "Then check distance function  [euclidean > gaussian > inner_prod > cosine_dist];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model: <hw1_knn.KNN object at 0x613559390>\n",
      "best_k: 13\n",
      "best_function: gaussian\n",
      "best_scaler: min_max_scale\n"
     ]
    }
   ],
   "source": [
    "best_model, best_k, best_function = model_selection_without_normalization(distance_funcs, Xtrain, ytrain, Xval, yval)\n",
    "print(\"best_model:\",best_model)\n",
    "print(\"best_k:\",best_k)\n",
    "print(\"best_function:\",best_function)\n",
    "print(\"best_scaler:\",best_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data RMSE: 0.625\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler(Xtest)\n",
    "Y_pred = best_model.predict(X)\n",
    "\n",
    "print(\"test data RMSE:\",mean_squared_error(ytest, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Data transformation\n",
    "\n",
    "Here, we take two different data transformation approaches.\n",
    "\n",
    "#### Normalizing the feature vector \n",
    "\n",
    "This one is simple but some times may work well. Given a feature vector $x$, the normalized feature vector is given by \n",
    "\n",
    "$$ x' = \\frac x {\\sqrt{\\langle x, x \\rangle}} $$\n",
    "If a vector is a all-zero vector, we let the normalized vector also be a all-zero vector.\n",
    "\n",
    "1.normalize\n",
    "    \n",
    "normalize the feature vector for each sample . For example, if the input features = [[3, 4], [1, -1], [0, 0]], the output should be [[0.6, 0.8], [0.707107, -0.707107], [0, 0]]\n",
    "        \n",
    "#### Min-max scaling the feature matrix\n",
    "\n",
    "The above normalization is data independent, that is to say, the output of the normalization function doesn’t depend on the rest training data. However, sometimes it would be helpful to do data dependent normalization. One thing to note is that, when doing data dependent normalization, we can only use training data, as the test data is assumed to be unknown during training (at least for most classification tasks).\n",
    "\n",
    "The min-max scaling works as follows: after min-max scaling, all values of training data’s feature vectors are in the given range.\n",
    "Note that this doesn’t mean the values of the validation/test data’s fea- tures are all in that range, because the validation/test data may have dif- ferent distribution as the training data.\n",
    "\n",
    "2.min_max_scale\n",
    "\n",
    "normalize the feature vector for each sample . For example, if the input features = [[2, -1], [-1, 5], [0, 0]], the output should be [[1, 0], [0, 1], [0.333333, 0.16667]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import NormalizationScaler, MinMaxScaler\n",
    "\n",
    "scaling_classes = {\n",
    "    'min_max_scale': MinMaxScaler,\n",
    "    'normalize': NormalizationScaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model: <hw1_knn.KNN object at 0x1a14ace898>\n",
      "best_k: 25\n",
      "best_function: euclidean\n",
      "best_scaler: min_max_scale\n"
     ]
    }
   ],
   "source": [
    "best_model, best_k, best_function, best_scaler = model_selection_with_transformation(distance_funcs, scaling_classes, Xtrain, ytrain, Xval, yval)\n",
    "print(\"best_model:\",best_model)\n",
    "print(\"best_k:\",best_k)\n",
    "print(\"best_function:\",best_function)\n",
    "print(\"best_scaler:\",best_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data RMSE: 0.25\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler(Xtest)\n",
    "Y_pred = best_model.predict(X)\n",
    "\n",
    "print(\"test data RMSE:\",mean_squared_error(ytest, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE for test data prediction was improved from 0.625 to 0.25\n",
    "which shows that appropriate scaling and normalization can enhance the performance of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
