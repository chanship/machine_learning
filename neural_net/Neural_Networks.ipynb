{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "<img src=\"MLP_diagram.png\">\n",
    "<br><br>\n",
    "Using this neural network to classify MNIST database of handwritten digits (0-9). The architecture of the multi-layer perceptron (MLP, just another term for fully connected feedforward networks) for a K-class classification problem. \n",
    "\n",
    "Let $(x\\in\\mathbb{R}^D, y\\in\\{1,2,\\cdots,K\\})$ be a labeled instance, such an MLP performs the following computations.\n",
    "<br><br><br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    " \\textbf{input features}: \\hspace{15pt} & x \\in \\mathbb{R}^D \\\\\n",
    " \\textbf{linear}^{(1)}: \\hspace{15pt} & u = W^{(1)}x + b^{(1)} \\hspace{2em}, W^{(1)} \\in \\mathbb{R}^{M\\times D} \\text{ and } b^{(1)} \\in \\mathbb{R}^{M}  \\label{linear_forward}\\\\\n",
    " \\textbf{tanh}:\\hspace{15pt} & h =\\cfrac{2}{1+e^{-2u}}-1 \\label{tanh_forward}\\\\\n",
    " \\textbf{relu}: \\hspace{15pt} & h = max\\{0, u\\} =\n",
    "\\begin{bmatrix}\n",
    "\\max\\{0, u_1\\}\\\\\n",
    "\\vdots \\\\\n",
    "\\max\\{0, u_M\\}\\\\\n",
    "\\end{bmatrix} \\label{relu_forward}\\\\\n",
    " \\textbf{linear}^{(2)}: \\hspace{15pt} & a = W^{(2)}h + b^{(2)} \\hspace{2em}, W^{(2)} \\in \\mathbb{R}^{K\\times M} \\text{ and } b^{(2)} \\in \\mathbb{R}^{K} \\label{linear2_forward}\\\\\n",
    " \\textbf{softmax}: \\hspace{15pt} & z = \\begin{bmatrix}\n",
    "\\cfrac{e^{a_1}}{\\sum_{k} e^{a_{k}}}\\\\\n",
    "\\vdots \\\\\n",
    "\\cfrac{e^{a_K}}{\\sum_{k} e^{a_{k}}} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    " \\textbf{predicted label}: \\hspace{15pt} & \\hat{y} = argmax_k z_k.\n",
    "%& l = -\\sum_{k} y_{k}\\log{\\hat{y_{k}}} \\hspace{2em}, \\vy \\in \\mathbb{R}^{k} \\text{ and } y_k=1 \\text{ if } \\vx \\text{ belongs to the } k' \\text{-th class}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "For a $K$-class classification problem, one popular loss function for training (i.e., to learn $W^{(1)}$, $W^{(2)}$, $b^{(1)}$, $b^{(2)}$) is the cross-entropy loss.\n",
    "Specifically we denote the cross-entropy loss with respect to the training example $(x, y)$ by $l$:\n",
    "<br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    "  l = -\\log (z_y) = \\log \\left( 1 + \\sum_{k\\neq y} e^{a_k - a_y} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "<br><br>\n",
    "Note that one should look at $l$ as a function of the parameters of the network, that is, $W^{(1)}, b^{(1)}, W^{(2)}$ and $b^{(2)}$.\n",
    "For ease of notation, let us define the one-hot (i.e., 1-of-$K$) encoding of a class $y$ as\n",
    "\n",
    "\\begin{align}\n",
    "y \\in \\mathbb{R}^K \\text{ and }\n",
    "y_k =\n",
    "\\begin{cases}\n",
    "1, \\text{ if }y = k,\\\\\n",
    "0, \\text{ otherwise}.\n",
    "\\end{cases} \n",
    "\\end{align}\n",
    "so that\n",
    "\\begin{align} \n",
    "l = -\\sum_{k} y_{k}\\log{z_k} = \n",
    "-y^T\n",
    "\\begin{bmatrix}\n",
    "\\log z_1\\\\\n",
    "\\vdots \\\\\n",
    "\\log z_K\\\\\n",
    "\\end{bmatrix}\n",
    "= -y^T\\log{z}.\n",
    "\\end{align}\n",
    "\n",
    "Then perform error-backpropagation, a way to compute partial derivatives (or gradients) w.r.t the parameters of a neural network, and use gradient-based optimization to learn the parameters.  \n",
    "\n",
    "### 1. Mini batch Gradient Descent \n",
    "Mini-batch gradient descent which is a gradient-based optimization to learn the parameters of the neural network. \n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\upsilon = \\alpha \\upsilon - \\eta \\delta_t\\\\\n",
    "w_t = w_{t-1} + \\upsilon\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "Use the formula above to update the weights using momentum. <br>\n",
    "Here,\n",
    "$\\alpha$ is the discount factor such that $\\alpha \\in (0, 1)$ <br>\n",
    "$\\upsilon$ is the velocity update<br>\n",
    "$\\eta$ is the learning rate<br>\n",
    "$\\delta_t$ is the gradient<br>\n",
    "\n",
    "### 2. Linear Layer\n",
    "The linear layer of MLP. Initialize W with random values using np.random.normal such that the mean is 0 and standard deviation is 0.1. Or initialize gradients to zeroes in the same function. Compute gradients of W and b in backward pass. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{forward pass:}\\hspace{2em} &\n",
    "u = \\text{linear}^{(1)}\\text{.forward}(x) = W^{(1)}x + b^{(1)},\\\\\n",
    "&\\text{where } W^{(1)} \\text{ and } b^{(1)} \\text{ are its parameters.}\\nonumber\\\\ \n",
    "\\nonumber\\\\\n",
    "\\text{backward pass:}\\hspace{2em} &[\\frac{\\partial l}{\\partial x}, \\frac{\\partial l}{\\partial W^{(1)}}, \\frac{\\partial l}{\\partial b^{(1)}}] = \\text{linear}^{(1)}\\text{.backward}(x, \\frac{\\partial l}{\\partial u}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 3. Activation function - tanh\n",
    "The activation function tanh. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{tanh}:\\hspace{15pt} & h =\\cfrac{2}{1+e^{-2u}}-1\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 4. Activation function - relu\n",
    "Another activation function called relu. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{relu}: \\hspace{15pt} & h = max\\{0, u\\} =\n",
    "\\begin{bmatrix}\n",
    "\\max\\{0, u_1\\}\\\\\n",
    "\\vdots \\\\\n",
    "\\max\\{0, u_M\\}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 5. Dropout\n",
    "To prevent overfitting, we usually add regularization. Dropout is another way of handling overfitting. We define the forward and the backward passes as follows.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{forward pass:}\\hspace{2em} &\n",
    "{s} = \\text{dropout}\\text{.forward}({q}\\in\\mathbb{R}^J) = \\frac{1}{1-r}\\times\n",
    "\\begin{bmatrix}\n",
    "\\textbf{1}[p_1 >= r] \\times q_1\\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{1}[p_J >= r] \\times q_J\\\\\n",
    "\\end{bmatrix},\n",
    "\\\\\n",
    "\\nonumber\\\\\n",
    "&\\text{where } p_j \\text{ is sampled uniformly from }[0, 1), \\forall j\\in\\{1,\\cdots,J\\}, \\nonumber\\\\\n",
    "&\\text{and } r\\in [0, 1) \\text{ is a pre-defined scalar named dropout rate}.\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\text{backward pass:}\\hspace{2em} &\\frac{\\partial l}{\\partial {q}} = \\text{dropout}\\text{.backward}({q}, \\frac{\\partial l}{\\partial {s}})=\n",
    "\\frac{1}{1-r}\\times\n",
    "\\begin{bmatrix}\n",
    "\\textbf{1}[p_1 >= r] \\times \\cfrac{\\partial l}{\\partial s_1}\\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{1}[p_J >= r] \\times \\cfrac{\\partial l}{\\partial s_J}\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Note that $p_j, j\\in\\{1,\\cdots,J\\}$ and $r$ are not be learned so we do not need to compute the derivatives w.r.t. to them. Moreover, $p_j, j\\in\\{1,\\cdots,J\\}$ are re-sampled every forward pass, and are kept for the following backward pass. The dropout rate $r$ is set to 0 during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1\n",
      "Training loss at epoch 1 is 330.1989480383847\n",
      "Training accuracy at epoch 1 is 0.8992\n",
      "Validation accuracy at epoch 1 is 0.903\n",
      "At epoch 2\n",
      "Training loss at epoch 2 is 214.72871312475223\n",
      "Training accuracy at epoch 2 is 0.9426\n",
      "Validation accuracy at epoch 2 is 0.928\n",
      "At epoch 3\n",
      "Training loss at epoch 3 is 158.3293031904497\n",
      "Training accuracy at epoch 3 is 0.956\n",
      "Validation accuracy at epoch 3 is 0.932\n",
      "At epoch 4\n",
      "Training loss at epoch 4 is 122.53548073976626\n",
      "Training accuracy at epoch 4 is 0.9732\n",
      "Validation accuracy at epoch 4 is 0.936\n",
      "At epoch 5\n",
      "Training loss at epoch 5 is 101.20980823990539\n",
      "Training accuracy at epoch 5 is 0.9802\n",
      "Validation accuracy at epoch 5 is 0.941\n",
      "At epoch 6\n",
      "Training loss at epoch 6 is 78.78150386882093\n",
      "Training accuracy at epoch 6 is 0.9874\n",
      "Validation accuracy at epoch 6 is 0.942\n",
      "At epoch 7\n",
      "Training loss at epoch 7 is 68.65701316349937\n",
      "Training accuracy at epoch 7 is 0.9898\n",
      "Validation accuracy at epoch 7 is 0.947\n",
      "At epoch 8\n",
      "Training loss at epoch 8 is 59.54833394999111\n",
      "Training accuracy at epoch 8 is 0.9912\n",
      "Validation accuracy at epoch 8 is 0.95\n",
      "At epoch 9\n",
      "Training loss at epoch 9 is 46.89296297058256\n",
      "Training accuracy at epoch 9 is 0.997\n",
      "Validation accuracy at epoch 9 is 0.95\n",
      "At epoch 10\n",
      "Training loss at epoch 10 is 43.517096785973166\n",
      "Training accuracy at epoch 10 is 0.9982\n",
      "Validation accuracy at epoch 10 is 0.956\n",
      "Finish running!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([330.1989480383847,\n",
       "  214.72871312475223,\n",
       "  158.3293031904497,\n",
       "  122.53548073976626,\n",
       "  101.20980823990539,\n",
       "  78.78150386882093,\n",
       "  68.65701316349937,\n",
       "  59.54833394999111,\n",
       "  46.89296297058256,\n",
       "  43.517096785973166],\n",
       " [65.50061441820793,\n",
       "  50.84967263201994,\n",
       "  45.51357273448952,\n",
       "  43.09664852512866,\n",
       "  41.93889203204472,\n",
       "  39.006270866201,\n",
       "  37.51718816295636,\n",
       "  35.69871360326422,\n",
       "  35.70158095571034,\n",
       "  35.10064515601373])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "from neural_networks import main, miniBatchGradientDescent, dropout, tanh, relu, linear_layer\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--random_seed', default=42)\n",
    "parser.add_argument('--learning_rate', default=0.01)\n",
    "parser.add_argument('--alpha', default=0.0)\n",
    "parser.add_argument('--lambda', default=0.0)\n",
    "parser.add_argument('--dropout_rate', default=0.0)\n",
    "parser.add_argument('--num_epoch', default=10)\n",
    "parser.add_argument('--minibatch_size', default=5)\n",
    "parser.add_argument('--activation', default='relu')\n",
    "parser.add_argument('--input_file', default='mnist_subset.json')\n",
    "args = parser.parse_args()\n",
    "main_params = vars(args)\n",
    "main(main_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1\n",
      "Training loss at epoch 1 is 1485.9423066914571\n",
      "Training accuracy at epoch 1 is 0.815\n",
      "Validation accuracy at epoch 1 is 0.83\n",
      "At epoch 2\n",
      "Training loss at epoch 2 is 948.4358639759693\n",
      "Training accuracy at epoch 2 is 0.8904\n",
      "Validation accuracy at epoch 2 is 0.88\n",
      "At epoch 3\n",
      "Training loss at epoch 3 is 861.7931225157394\n",
      "Training accuracy at epoch 3 is 0.9076\n",
      "Validation accuracy at epoch 3 is 0.891\n",
      "At epoch 4\n",
      "Training loss at epoch 4 is 465.93812790020087\n",
      "Training accuracy at epoch 4 is 0.945\n",
      "Validation accuracy at epoch 4 is 0.925\n",
      "At epoch 5\n",
      "Training loss at epoch 5 is 865.0176855759477\n",
      "Training accuracy at epoch 5 is 0.9118\n",
      "Validation accuracy at epoch 5 is 0.884\n",
      "At epoch 6\n",
      "Training loss at epoch 6 is 396.8032749306144\n",
      "Training accuracy at epoch 6 is 0.958\n",
      "Validation accuracy at epoch 6 is 0.928\n",
      "At epoch 7\n",
      "Training loss at epoch 7 is 354.224859912969\n",
      "Training accuracy at epoch 7 is 0.9592\n",
      "Validation accuracy at epoch 7 is 0.942\n",
      "At epoch 8\n",
      "Training loss at epoch 8 is 892.1045177979262\n",
      "Training accuracy at epoch 8 is 0.9072\n",
      "Validation accuracy at epoch 8 is 0.876\n",
      "At epoch 9\n",
      "Training loss at epoch 9 is 320.97949501683354\n",
      "Training accuracy at epoch 9 is 0.963\n",
      "Validation accuracy at epoch 9 is 0.926\n",
      "At epoch 10\n",
      "Training loss at epoch 10 is 157.1995671705035\n",
      "Training accuracy at epoch 10 is 0.9778\n",
      "Validation accuracy at epoch 10 is 0.938\n",
      "Finish running!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1485.9423066914571,\n",
       "  948.4358639759693,\n",
       "  861.7931225157394,\n",
       "  465.93812790020087,\n",
       "  865.0176855759477,\n",
       "  396.8032749306144,\n",
       "  354.224859912969,\n",
       "  892.1045177979262,\n",
       "  320.97949501683354,\n",
       "  157.1995671705035],\n",
       " [282.82032361844574,\n",
       "  231.6451654596812,\n",
       "  244.9950386003027,\n",
       "  186.53752665336742,\n",
       "  259.0499962866089,\n",
       "  191.25386829647206,\n",
       "  145.43853640120957,\n",
       "  299.6363552804344,\n",
       "  224.11053640911814,\n",
       "  198.27128976289993])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--random_seed', default=42)\n",
    "parser.add_argument('--learning_rate', default=0.01)\n",
    "parser.add_argument('--alpha', default=0.9)\n",
    "parser.add_argument('--lambda', default=0.0)\n",
    "parser.add_argument('--dropout_rate', default=0.25)\n",
    "parser.add_argument('--num_epoch', default=10)\n",
    "parser.add_argument('--minibatch_size', default=5)\n",
    "parser.add_argument('--activation', default='tanh')\n",
    "parser.add_argument('--input_file', default='mnist_subset.json')\n",
    "args = parser.parse_args()\n",
    "main_params = vars(args)\n",
    "main(main_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
