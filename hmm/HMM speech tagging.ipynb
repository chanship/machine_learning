{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "calculate HMM parameters by yourself and use it to solve Part-of-Speech Tagging problem.\n",
    "\n",
    "Two algorithms usually be used for evaluation problem: the forward algorithm or the backwards algorithm (DO NOT confuse them with the forward-backward algorithm).\n",
    "\n",
    "### HMM Class\n",
    "In this project, we abstracted Hiddern Markov Model as a class. Each Hiddern Markov Model initialized with Pi, A, B, obs_dict and state_dict. HMM class has 5 inner functions: Forward function, backward function, sequence_prob function, posterior_prob function and viterbi function.\n",
    "```\n",
    "    class HMM:\n",
    "    def __init__(self, pi, A, B, obs_dict, state_dict):\n",
    "        - pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(Z_1 = s_i)\n",
    "        - A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(Z_t = s_j|Z_t-1 = s_i)\n",
    "        - B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, k] = P(O_t = x_k| Z_t = s_i)\n",
    "        - obs_dict: A dictionary mapping each observation symbol to their index in B\n",
    "        - state_dict: A dictionary mapping each state to their index in pi and A\n",
    "    # TODO:\n",
    "    def forward(self, Osequence):\n",
    "    # TODO:\n",
    "    def backward(self, Osequence):\n",
    "    # TODO:\n",
    "    def sequence_prob(self, Osequence):\n",
    "    # TODO:\n",
    "    def posterior_prob(self, Osequence):\n",
    "    # TODO:\n",
    "    def viterbi(self, Osequence):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation problem\n",
    "#### (a) Forward algorithm and backward algorithm\n",
    "- $\\delta[i, t] = \\delta_t(i) = P(Z_t = s_i, x_{1:t} | \\lambda).$\n",
    "```\n",
    "def forward(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - self.pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(Z_1 = s_i)\n",
    "    - self.A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(Z_t = s_j|Z_t-1 = s_i)\n",
    "    - self.B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, k] = P(O_t = x_k| Z_t = s_i)\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - delta: (num_state*L) A numpy array delta[i, t] = P(Z_t = s_i, x_1:x_t | λ)\n",
    "    \"\"\"\n",
    "```\n",
    "- $\\gamma[i, t] = \\gamma_t(i) = P(x_{t+1:T}|Z_t = s_i, \\lambda).$\n",
    "```\n",
    "def backward(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - self.pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(Z_1 = s_i)\n",
    "    - self.A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(Z_t = s_j|Z_t-1 = s_i)\n",
    "    - self.B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, k] = P(O_t = x_k| Z_t = s_i)\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - gamma: (num_state*L) A numpy array gamma[i, t] = P(x_t+1:x_T | Z_t = s_i, λ)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### (b) Sequence probability\n",
    "Based on your forward, backward function, calculate the sequence probability.\n",
    "$$P(x_1, . . . , x_L = O|\\lambda) = \\sum_{i=1}^{N}P(Z_t = s_i, x_{1:T} | \\lambda) = \\sum_{i=1}^{N}\\delta[i, T]$$\n",
    "```\n",
    "def sequence_prob(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - prob: A float number of P(x_1:x_T | λ)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### (c) Posterior probability\n",
    "The forward variable $\\delta[i, t]$ and backward variable $\\gamma[i, t]$ are used to calculate the posterior probability of a specific case. Now for t = 1...T and i = 1...N, we define posterior probability $\\beta_t(i) = P(Z_t = s_i|O,\\lambda)$ the probability of being in state $s_t=i$ at time t given the observation sequence O and the model $\\lambda$.<br>\n",
    "$$\\beta_t(i) = \\frac{P(Z_t = s_i, O|\\lambda)}{P(O|\\lambda)} = \\frac{P(Z_t = s_i, x_{1:t}|\\lambda)}{P(O|\\lambda)}$$<br>\n",
    "$$P(Z_t = s_i, x_{1:t}|\\lambda) = P(x_{1:t}|Z_t = s_i,\\lambda) \\cdot P(x_{t+1:T}|Z_t = s_i,\\lambda) \\cdot P(Z_t = s_i|\\lambda) = \\delta[i, t] \\cdot \\gamma[i, t]$$\n",
    "Thus<br>\n",
    "$$\\beta_t(i) = \\frac{\\delta[i, t] \\cdot \\gamma[i, t]}{P(O|\\lambda)}$$\n",
    "where<br>\n",
    "$$P(O|\\lambda) = \\sum_{i=1}^{N}\\delta[i, T]$$\n",
    "Signature:\n",
    "```\n",
    "def posterior_prob(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - prob: (num_state*L) A numpy array of P(s_t = i|O, λ)\n",
    "    \"\"\"\n",
    "```\n",
    "You can use $\\beta_t(i)$ to find the most likely state at time t which is the state $Z_t=s_i$ for which $\\beta_t(i)$ is maximum. This algorithm works fine in the case when HMM is ergodic i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem Viterbi algorithm is the most common decoding algorithms used.\n",
    "\n",
    "\n",
    "#### (d) Viterbi algorithm\n",
    "Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states. We want to compute the most likely state path that corresponds to the observation sequence O based HMM. Namely, $k^∗ = (k^∗_1,k^∗_2,··· ,k^∗_L) = argmax_k P(s_{k_1},s_{k_2},··· ,s_{k_L}|x_1,x_2,··· ,x_L = O, \\lambda)$. <br>\n",
    "Signature:\n",
    "```\n",
    "def viterbi(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - path: A List of the most likely hidden state path k* (return state instead of idx)\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your forward function output: [[3.5000e-01 1.3600e-01 0.0000e+00 0.0000e+00 1.1136e-05 1.1136e-05\n",
      "  0.0000e+00]\n",
      " [1.5000e-01 3.2000e-02 4.6400e-03 2.7840e-04 3.3408e-05 1.1136e-05\n",
      "  8.9088e-07]]\n",
      "My forward function output: [[0.35, 0.136, 0.0, 0.0, 1.1136e-05, 1.1136e-05, 0.0], [0.15, 0.032, 0.00464, 0.0002784, 3.3408e-05, 1.1136e-05, 8.9088e-07]]\n",
      "Your backward function output: [[1.6896e-06 3.8400e-06 6.4000e-05 2.0000e-03 1.4000e-02 2.0000e-02\n",
      "  1.0000e+00]\n",
      " [1.9968e-06 1.1520e-05 1.9200e-04 3.2000e-03 2.2000e-02 6.0000e-02\n",
      "  1.0000e+00]]\n",
      "My backward function output: [[1.6896e-06, 3.84e-06, 6.4e-05, 0.002, 0.014, 0.02, 1.0], [1.9968e-06, 1.152e-05, 0.000192, 0.0032, 0.022, 0.06, 1.0]]\n",
      "Your sequence_prob function output: 8.908800000000001e-07\n",
      "My sequence_prob function output: [[0.6637931, 0.5862069, 0.0, 0.0, 0.175, 0.25, 0.0], [0.3362069, 0.4137931, 1.0, 1.0, 0.825, 0.75, 1.0]]\n",
      "Your posterior_prob function output: [[0.6637931 0.5862069 0.        0.        0.175     0.25      0.       ]\n",
      " [0.3362069 0.4137931 1.        1.        0.825     0.75      1.       ]]\n",
      "My posterior_prob function output: [[0.6637931, 0.5862069, 0.0, 0.0, 0.175, 0.25, 0.0], [0.3362069, 0.4137931, 1.0, 1.0, 0.825, 0.75, 1.0]]\n",
      "Your viterbi function output:  ['1', '1', '2', '2', '2', '2', '2']\n",
      "My viterbi function output:  ['1', '1', '2', '2', '2', '2', '2']\n",
      "\n",
      "hmm total time:  0.006579875946044922\n"
     ]
    }
   ],
   "source": [
    "from hmm_test_script import hmm_test, speech_tagging_test\n",
    "\n",
    "hmm_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application to Speech Tagging\n",
    "Part-of-Speech (POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties.(noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article, or determiner.)<br>\n",
    "Part-of-Speech Tagging (POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.\n",
    "\n",
    "### 2.1. Dataset\n",
    "tags.txt: Universal Part-of-Speech Tagset\n",
    "\n",
    "| Tag | Meaning | English Examples |\n",
    "| :------ | :------ | :------ |\n",
    "| ADJ | adjective | new, good, high, special, big, local |\n",
    "| ADP | adposition | on, of, at, with, by, into, under |\n",
    "| ADV | adverb | really, already, still, early, now |\n",
    "| CONJ | conjunction | and, or, but, if, while, although |\n",
    "| DET | determiner, article | the, a, some, most, every, no, which |\n",
    "| NOUN | noun | year, home, costs, time, Africa |\n",
    "| NUM | numeral | twenty-four, fourth, 1991, 14:24 |\n",
    "| PRT | particle | at, on, out, over per, that, up, with |\n",
    "| PRON | pronoun | he, their, her, its, my, I, us |\n",
    "| VERB | verb | is, say, told, given, playing, would |\n",
    "| . | punctuation marks | . , ; ! |\n",
    "| X | other | ersatz, esprit, dunno, gr8, univeristy |\n",
    "\n",
    "sentences.txt: Including 57340 sentences which have already been tagged.<br>\n",
    "\n",
    "| Word | Tag |\n",
    "|:------:|:------:|\n",
    "| b100-48585 |\n",
    "| She | PRON |\n",
    "| had | VERB |\n",
    "| to | PRT |\n",
    "| move | VERB |\n",
    "| in | ADP |\n",
    "| some | DET |\n",
    "| direction | NOUN |\n",
    "| -- | . |\n",
    "| any | DET |\n",
    "| direction | NOUN |\n",
    "| that | PRON |\n",
    "| would | VERB |\n",
    "| take | VERB |\n",
    "| her | PRON |\n",
    "| away | ADV |\n",
    "| from | ADP |\n",
    "| this | DET |\n",
    "| evil | ADJ |\n",
    "| place | NOUN |\n",
    "| . | . |\n",
    "\n",
    "### 2.2. Part-of-Speech Tagging\n",
    "In this part, we collect our dataset and tags with Dataset class. Dataset class includes tags, train_data and test_data. In both dataset include a list of senetences, each sentence is an object of Line class.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Speech_tagging\n",
    "Based on HMM from 2.2, do speech tagging for each sentence on test data. Note when you meet a word which is unseen in training dataset. You need to modify the emission matrix and obs_dict of your current model in order to handle this case. You  will assume the emission probability from each state to a new unseen word is 10^-6(a very low probability).<br>\n",
    "```\n",
    "For example, in hmm_model.json, we use the following paramaters to initialize HMM:\n",
    "S = [\"1\", \"2\"]\n",
    "pi: [0.7, 0.3]\n",
    "A: [[0.8, 0.2], [0.4, 0.6]]\n",
    "B = [[0.5, 0, 0.4, 0.1], [0.5, 0.1, 0.2, 0.2]]\n",
    "Observations = [\"A\", \"C\", \"G\", \"T\"]\n",
    "If we find another observation symbol \"X\" in observation sequence, we will modify parameters of HMM as follows:\n",
    "S = [\"1\", \"2\"]\n",
    "pi: [0.7, 0.3]\n",
    "A: [[0.8, 0.2], [0.4, 0.6]]\n",
    "B = [[0.5, 0, 0.4, 0.1, 1e-6], [0.5, 0.1, 0.2, 0.2, 1e-6]]\n",
    "Observations = [\"A\", \"C\", \"G\", \"T\", \"X\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  ['DET', 'NOUN', '.', 'NOUN', 'CONJ', 'VERB']\n",
      "true:  ['ADJ', 'NOUN', '.', 'NOUN', 'CONJ', 'NOUN']\n",
      "accuracy:  0.6666666666666666\n",
      "pred:  ['CONJ', 'VERB', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN', '.', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "true:  ['CONJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PRT', 'VERB', '.', 'ADV', 'ADV', 'PRT', 'VERB', '.']\n",
      "accuracy:  0.46153846153846156\n",
      "pred:  ['.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', '.', 'VERB', 'ADP', 'DET', 'NOUN', '.', 'CONJ', 'ADJ', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "true:  ['.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', '.', 'VERB', 'PRT', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', '.']\n",
      "accuracy:  0.6956521739130435\n",
      "pred:  ['DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.', 'PRT', 'ADP', 'NOUN', 'CONJ', 'VERB', 'ADP', 'NOUN', 'CONJ', 'DET', 'NOUN', 'ADP', 'PRT', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', '.', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'CONJ', '.', 'ADP', 'DET', 'ADJ', 'NUM', 'ADP', 'NOUN', '.', 'VERB', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'NOUN', '.']\n",
      "true:  ['DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'CONJ', 'NOUN', 'ADP', 'NOUN', 'CONJ', 'DET', 'NOUN', 'ADP', 'PRT', 'ADJ', 'NOUN', 'VERB', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'NOUN', 'NOUN', 'CONJ', '.', 'ADP', 'DET', 'ADJ', 'NUM', 'NOUN', 'NOUN', '.', 'VERB', 'VERB', 'ADP', 'NOUN', 'NOUN', 'NUM', '.', 'NUM', '.']\n",
      "accuracy:  0.64\n",
      "pred:  ['ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', '.', 'PRON', 'VERB', 'ADV', 'VERB', 'PRT', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'CONJ', 'ADP', 'NOUN', '.']\n",
      "true:  ['ADV', 'DET', 'NOUN', 'NOUN', 'VERB', 'ADV', 'ADJ', 'ADP', 'NOUN', 'PRT', 'VERB', 'NOUN', 'PRON', 'VERB', 'ADV', 'VERB', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'CONJ', 'ADP', 'NOUN', '.']\n",
      "accuracy:  0.6551724137931034\n",
      "pred:  ['DET', 'NOUN', '.', 'PRON', 'PRON', 'VERB', 'ADV', 'ADJ', '.']\n",
      "true:  ['DET', 'NOUN', 'VERB', 'PRON', 'PRON', 'VERB', 'ADV', 'ADJ', '.']\n",
      "accuracy:  0.8888888888888888\n",
      "pred:  ['DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'PRON', 'ADP', 'DET', 'ADJ', 'NOUN', '.']\n",
      "true:  ['DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NUM', 'VERB', 'ADP', 'VERB', 'VERB', 'VERB', 'VERB', 'ADP', 'PRON', 'ADP', 'DET', 'ADJ', 'NOUN', '.']\n",
      "accuracy:  0.7083333333333334\n",
      "pred:  ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.']\n",
      "true:  ['PRON', 'VERB', 'VERB', 'ADV', 'ADP', 'DET', 'ADJ', 'NOUN', '.']\n",
      "accuracy:  0.7777777777777778\n",
      "pred:  ['DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NUM', '.']\n",
      "true:  ['ADJ', 'NOUN', 'ADP', 'NUM', 'VERB', 'ADV', 'NOUN', 'NUM', '.']\n",
      "accuracy:  0.4444444444444444\n",
      "pred:  ['PRT', 'VERB', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADV', 'PRON', 'VERB', 'NOUN', '.']\n",
      "true:  ['PRT', 'VERB', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADV', 'PRON', 'VERB', 'NOUN', '.']\n",
      "accuracy:  1.0\n",
      "Your total accuracy:  0.6793478260869565\n",
      "My total accuracy:  0.6793478260869565\n",
      "speech_tagging total time:  1.467008113861084\n"
     ]
    }
   ],
   "source": [
    "speech_tagging_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
